# Audit Service Configuration
# Copy this file to .env and fill in your values

# ===== Server Configuration =====
AUDIT_HOST=0.0.0.0
AUDIT_PORT=8080

# ===== LLM Configuration =====
# Enable or disable LLM-powered analysis
LLM_ENABLED=true

# LLM provider (currently supports: grok)
LLM_PROVIDER=grok

# Grok API key from https://x.ai
# Get your key at: https://console.x.ai/
XAI_API_KEY=your-grok-api-key-here

# Model to use (grok-4-1-fast-reasoning for 2M context window)
LLM_MODEL=grok-4-1-fast-reasoning

# Maximum tokens for LLM responses
LLM_MAX_TOKENS=4096

# Temperature for LLM sampling (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.7

# ===== Git Configuration =====
# Directory where repositories are cloned
GIT_WORKSPACE_DIR=./workspace

# Default branch to checkout
GIT_DEFAULT_BRANCH=main

# Use shallow clones for faster cloning (depth=1)
GIT_SHALLOW_CLONE=true

# ===== Scanner Configuration =====
# Maximum file size to scan in bytes (1MB default)
SCANNER_MAX_FILE_SIZE=1000000

# Include test files in scans
SCANNER_INCLUDE_TESTS=false

# ===== Storage Configuration =====
# Directory where audit reports are saved
STORAGE_REPORTS_DIR=./reports

# Directory where generated tasks are saved
STORAGE_TASKS_DIR=./tasks

# ===== Logging =====
# Log level: trace, debug, info, warn, error
RUST_LOG=info

# ===== Optional: Advanced Settings =====
# Uncomment and configure as needed

# Rate limiting for LLM API calls
# LLM_RATE_LIMIT=10

# Timeout for LLM API calls in seconds
# LLM_TIMEOUT=60

# Enable caching for LLM responses
# LLM_CACHE_ENABLED=true

# Cache directory
# LLM_CACHE_DIR=./cache
